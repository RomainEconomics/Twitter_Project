{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import twint\n",
    "import nest_asyncio # Nécessaire pour la librairie Twint\n",
    "from unidecode import unidecode\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Présentation du Projet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans le cadre du cours text mining, nous avons travaillé sur des données issues de Twitter concernant le projet avorté de la super ligue européenne. Ce projet est l'occasion d'aborder toutes les thématiques classique d'un projet de NLP.\n",
    "Ainsi, celui-ci se divise en quatre partie :\n",
    "- une première partie ou nous avons extrait les tweets qui concernaient notre sujet, puis effectuer du data cleaning pour filtrer les données qui n'apportaient pas d'informations supplémentaires.\n",
    "- une seconde partie ou l'accent a été mis sur la visualisation. Quels sont les acteurs en jeu ? Quels personnalités ou clubs se sont distingués dans cet évènement ? Et dans quelle proportion ?\n",
    "- une troisième partie ou on s'est attaché à modéliser nos données, et à faire des techniques de NLP, notamment concernant le topic modeling ou le clustering ainsi que via des modèles de deep learning utilisant la méthode du word embeddings \n",
    "- et une dernière partie, ou il s'est agi de créer une application streamlit permettant de mieux visualiser les résultats de nos modèles ainsi que de déployer notre application via le site [Herokuapp](https://streamlit-twitter-app.herokuapp.com/) grâce à la création également d'un répertoire sur le site GitHub."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Téléchargement des données à partir du module Twint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ainsi, pour effectuer notre recherche nous avons utilisé le hashtag superleague afin de récupérer les tweets qui nous intéressaient La librairie Twint nous évite de passer par l'API de Twitter, qui s'est révélée assez contraignante."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "config = twint.Config()\n",
    "config.Search = \"#superleague superleague\"\n",
    "config.Lang = \"fr\"\n",
    "config.Since = \"2021-04-16 00:00:00\"\n",
    "config.Until = \"2021-04-22 00:00:00\"\n",
    "config.Store_json = True\n",
    "config.Output = \"raw_tweets.json\"\n",
    "\n",
    "# La derniere ligne lance la recherche\n",
    "#twint.run.Search(config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Malgré que l'option *Lang = 'fr'* ait été choisie, de nombreux tweets ne sont pas en français. Il convient donc des les filtrer.\\\n",
    "Du fait de la taille du data frame, on procède par *chunk* de 10000 lignes, on séléctionne les tweets en français puis nous créons un nouveau fichier csv."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def preprocess_tweets():\n",
    "    chunks = pd.read_json('../Data_raw/raw_tweets.json', lines=True, chunksize = 10000)\n",
    "    for chunk in chunks:\n",
    "        result = chunk[chunk['language'] == 'fr']\n",
    "        result.to_csv('../Data_clean/tweets_fr.csv', index=False, header=True, mode='a')\n",
    "\n",
    "preprocess_tweets()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df = pd.read_csv('../Data_clean/tweets_fr.csv')\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(521394, 36)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Suppression des NaN et des doublons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# La fonction preprocess_tweets ajoute un header pour chaque chunk, il convient donc de le retirer\n",
    "df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True, errors='coerce')\n",
    "# Supprime les observations qui n'ont pas de date\n",
    "df = df[~df['date'].isnull()] \n",
    "\n",
    "print(f\"La table fait {df.shape[0]} lignes et {df.shape[1]} colonnes\" )\n",
    "print()\n",
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "La table fait 521052 lignes et 36 colonnes\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 521052 entries, 0 to 521393\n",
      "Data columns (total 36 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   id               521052 non-null  object        \n",
      " 1   conversation_id  521052 non-null  object        \n",
      " 2   created_at       521052 non-null  object        \n",
      " 3   date             521052 non-null  datetime64[ns]\n",
      " 4   time             521052 non-null  object        \n",
      " 5   timezone         521052 non-null  object        \n",
      " 6   user_id          521052 non-null  object        \n",
      " 7   username         521052 non-null  object        \n",
      " 8   name             521024 non-null  object        \n",
      " 9   place            56 non-null      object        \n",
      " 10  tweet            521052 non-null  object        \n",
      " 11  language         521052 non-null  object        \n",
      " 12  mentions         521052 non-null  object        \n",
      " 13  urls             521052 non-null  object        \n",
      " 14  photos           521052 non-null  object        \n",
      " 15  replies_count    521052 non-null  object        \n",
      " 16  retweets_count   521052 non-null  object        \n",
      " 17  likes_count      521052 non-null  object        \n",
      " 18  hashtags         521052 non-null  object        \n",
      " 19  cashtags         521052 non-null  object        \n",
      " 20  link             521052 non-null  object        \n",
      " 21  retweet          521052 non-null  object        \n",
      " 22  quote_url        58807 non-null   object        \n",
      " 23  video            521052 non-null  object        \n",
      " 24  thumbnail        157528 non-null  object        \n",
      " 25  near             0 non-null       object        \n",
      " 26  geo              0 non-null       object        \n",
      " 27  source           0 non-null       object        \n",
      " 28  user_rt_id       0 non-null       object        \n",
      " 29  user_rt          0 non-null       object        \n",
      " 30  retweet_id       0 non-null       object        \n",
      " 31  reply_to         521052 non-null  object        \n",
      " 32  retweet_date     0 non-null       object        \n",
      " 33  translate        0 non-null       object        \n",
      " 34  trans_src        0 non-null       object        \n",
      " 35  trans_dest       0 non-null       object        \n",
      "dtypes: datetime64[ns](1), object(35)\n",
      "memory usage: 147.1+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Supprime les colonnes qui ne sont composées que de NaN.\n",
    "df.dropna(axis=1, how='all',  inplace=True)\n",
    "\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(521052, 26)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# On supprime les tweets en double\n",
    "df = df[~df.duplicated()]\n",
    "df = df[~df['id'].duplicated()].reset_index()\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(37349, 27)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilisation de la liste des stop words issue de la librairie Spacy. Nous y avons ajouté quelques mots qui ne nous semblaient pas apporter de sens supplémentaires à notre projet. Les mots faisant référence à la superleague étant particulièrement redondant, car se trouvant forcément dans chaque tweet ont été supprimé. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "\n",
    "add_stop_words = ['europeansuperleague', \"superleague\", \"superleagu\",\"super\", 'foot', 'football','super', \"club\",\n",
    "                  'étaient', 'étais', 'était', 'étant', 'été', 'être', 'etre', 'etes', 'faire', 'aller', 'voir',\n",
    "                  'ô',  'à', 'â', 'ça', 'ès','a', 'b','c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', \n",
    "                  'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'qu']\n",
    "\n",
    "for word in add_stop_words:\n",
    "    STOP_WORDS.add(word)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def clubs_signataires(x, clubs):\n",
    "    \"\"\"\n",
    "    Création d'une fonction pour identifier les clubs  signataires, et faciliter le travail de \n",
    "    nos modèles de topic modeling.\n",
    "    \"\"\"\n",
    "    string = ' '.join(['clubs_signataires' if word  in clubs else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "signataires = ['arsenal', 'chelsea', 'chelseafc', 'chelseer', 'liverpool', \n",
    "                'manchestercity', 'mancity','manchester', 'manchesterunited', 'manutd', 'mufc',\n",
    "               'tottenham hotspur', 'tottenham', 'spur', 'spurs',\n",
    "                     'intermilan', 'juventus', 'juventu', 'juventusfc', 'juv', 'juve', 'turin', \n",
    "               'milan',  'acmilan','milanac', 'madrid', 'atletico', 'atleti', 'inter',\n",
    "               'barcelona', 'barcelone', 'barca', 'fcbarca', 'fcbarcelone', 'fcbarcelona',\n",
    "                'real', 'realmadrid', 'realmadridfrer']\n",
    "\n",
    "\n",
    "def clubs_non_signataires(x, clubs):\n",
    "    \"\"\"\n",
    "    Création d'une fonction pour identifier les clubs non signataires, et faciliter le travail de \n",
    "    nos modèles de topic modeling.\n",
    "    \"\"\"\n",
    "    string = ' '.join(['clubs_non_signataires' if word  in clubs else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "non_signataires = ['paris saint germain', 'psg', 'bayern munich', 'bayern', 'fcbayern',\n",
    "                   'munich', 'borussia', 'dortmund']\n",
    "\n",
    "\n",
    "def florentino(x, alias):\n",
    "    \"\"\"\n",
    "    Création d'une fonction pour identifier les différents alias utilisés pour parler de Florentino Perez et\n",
    "    permettre une meilleure identification de celui-ci\n",
    "    \"\"\"\n",
    "    string = ' '.join(['florentino_perez' if word  in alias else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "florentino_alias = ['perez', 'florentino', 'florentin', 'florentino_perer',\n",
    "                    'florentinoperer', 'florentinoperez', 'per', 'perer']\n",
    "\n",
    "\n",
    "def agnelli(x, alias):\n",
    "    \"\"\"\n",
    "    Création d'une fonction pour identifier les différents alias utilisés pour parler de Andreas Agnelli et\n",
    "    permettre une meilleure identification de celui-ci\n",
    "    \"\"\"\n",
    "    string = ' '.join(['andrea_agnelli' if word  in alias else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "agnelli_alias = ['andrea agnelli', 'andrea agneli', 'andrea', 'agnelli', 'agneli', 'andreer']\n",
    "\n",
    "\n",
    "\n",
    "def superleague_out(x, alias):\n",
    "    \"\"\"\n",
    "    Création d'une fonction pour identifier les différents alias utilisés pour exprimer le rejet du\n",
    "    projet de superleague\n",
    "    \"\"\"\n",
    "    string = ' '.join(['superleagueout' if word  in alias else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "slogans_out = ['superleagueout', 'saynotoeuropeansuperleagu', 'saynotosuperleagu', 'saynotosuperleague',\n",
    "      'notoeuropeansuperleagu','notoeuropeansuperleague', 'notosuperleagu', 'notosuperleague',\n",
    "      'nonalasuperleagu', 'boycottsuperleagu', 'boycottsuperleague']\n",
    "\n",
    "\n",
    "def suppression_alias(x, alias):\n",
    "    \"\"\"\n",
    "    Suppresion de valeurs redondantes qui ont normalement déjà étaient prises en comptes.\n",
    "    \"\"\"\n",
    "    string = ' '.join(['' if word  in alias else word for word in x.split() ])\n",
    "    return string\n",
    "\n",
    "sup_alias = ['paris saint germain', 'city', 'united', 'ac', 'fc', 'man'\n",
    "                   'rt', 'fav']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def tweet_cleaner(pandasSeries):\n",
    "    \"\"\"\n",
    "    Fonction ayant pour but de nettoyer nos tweets.\n",
    "    Prend comme paramètre un pandas series.\n",
    "    Opérations effectuées : mise au format string, minuscule, suppression des #, des mentions de compte,\n",
    "    des URL, puis lemmatisation de nos données via Spacy, et suppression des stop words. Ensuite,\n",
    "    traitement des données afin d'identifier les différents alias utilisés pour parler de clubs ou de personnalités.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"#### Nettoyage en cours ####\") \n",
    "    \n",
    "    # Confirmation que chaque article est bien de type str\n",
    "    pandasSeries = pandasSeries.apply(lambda x : str(x))\n",
    "    \n",
    "    # Passage en minuscule\n",
    "    print(\"... Passage en minuscule\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : x.lower())\n",
    "\n",
    "    # Suppression des #\n",
    "    print(\"... Suppression des #\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"#\", '', x))\n",
    "    \n",
    "    # Suppression des noms de compte #\n",
    "    print(\"... Suppression des @account\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"@\\S+\", '', x))\n",
    "    \n",
    "    # Suppression des url #\n",
    "    print(\"... Suppression des url\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"http\\S+\", '', x))\n",
    "    \n",
    "    # Lemmatisation\n",
    "    print(\"... Lemmatisation and Suppression des Stop words\")\n",
    "    \n",
    "    pandasSeries = pd.Series((nlp.pipe(pandasSeries)))\n",
    "    \n",
    "    tweets_clean = []\n",
    "    for tweet in pandasSeries:\n",
    "        tokens_clean = []\n",
    "        for token in tweet:\n",
    "            if (token.lemma_ not in STOP_WORDS):\n",
    "                tokens_clean.append((unidecode(token.lemma_)))\n",
    "        tweets_clean.append(' '.join(tokens_clean))\n",
    "    \n",
    "    pandasSeries = pd.Series(tweets_clean)\n",
    "    \n",
    "    ## Suppression des caractères spéciaux et numériques\n",
    "    print(\"... Suppression des caractères spéciaux et numériques\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"[^a-z]+\", ' ', x))\n",
    "    \n",
    "    print(\"... Identifications des clubs non signataires\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : clubs_non_signataires(x, non_signataires))    \n",
    "    \n",
    "    print(\"... Identifications des clubs signataires\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : clubs_signataires(x, signataires))\n",
    "    \n",
    "    print(\"... Identifications des différents alias pour Florentino Perez\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : florentino(x, florentino_alias))\n",
    "    \n",
    "    print(\"... Identifications des différents alias pour Andrea Agnelli\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : agnelli(x, agnelli_alias))\n",
    "    \n",
    "    print(\"... Identifications des différents termes qualifiant le rejet du projet\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : superleague_out(x, slogans_out))\n",
    "    \n",
    "    print(\"... Suppression d'alias\")\n",
    "    pandasSeries = pandasSeries.apply(lambda x : suppression_alias(x, sup_alias))\n",
    "    \n",
    "    print(\"#### Nettoyage OK! ####\")\n",
    "\n",
    "    return pandasSeries"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "%%time \n",
    "\n",
    "df['tweet_clean'] = tweet_cleaner(df['tweet'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#### Nettoyage en cours ####\n",
      "... Passage en minuscule\n",
      "... Suppression des #\n",
      "... Suppression des @account\n",
      "... Suppression des url\n",
      "... Lemmatisation and Suppression des Stop words\n",
      "... Suppression des caractères spéciaux et numériques\n",
      "... Identifications des clubs non signataires\n",
      "... Identifications des clubs signataires\n",
      "... Identifications des différents alias pour Florentino Perez\n",
      "... Identifications des différents alias pour Andrea Agnelli\n",
      "... Identifications des différents termes qualifiant le rejet du projet\n",
      "... Suppression d'alias\n",
      "#### Nettoyage OK! ####\n",
      "CPU times: user 1min 47s, sys: 1.83 s, total: 1min 48s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "df[df['tweet_clean'].isna()][['tweet']]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Création d'une colonne pour identifier les jours du mois\n",
    "df['day'] = df['date'].dt.day"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cols_to_keep = ['date', 'time', 'tweet', 'tweet_clean', 'hashtags', 'likes_count', 'retweets_count', 'day']\n",
    "\n",
    "tweets = df[cols_to_keep]\n",
    "tweets.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        date      time                                              tweet  \\\n",
       "0 2021-04-21  22:29:44  Jean Michel incisif face aux 12 frondeurs de l...   \n",
       "1 2021-04-21  22:29:01  Beppe #Marotta (AD Inter) lance un cri d’alarm...   \n",
       "2 2021-04-21  22:27:10  Avec l'échec du projet de #SuperLeague, la mor...   \n",
       "3 2021-04-21  22:26:00  Jean Michel trop heureux de la mort de la #Sup...   \n",
       "4 2021-04-21  22:25:48  3-1 pour la Juve contre Parme (2 Coupes UEFA) ...   \n",
       "\n",
       "                                         tweet_clean  \\\n",
       "0    jean michel incisif face frondeur teamom jamais   \n",
       "1  beppe marotta ad clubs_signataires lancer cri ...   \n",
       "2                     echec projet mort visage olasm   \n",
       "3               jean michel trop heureux mort teamom   \n",
       "4  clubs_signataires contre parme coupe uefa comp...   \n",
       "\n",
       "                     hashtags likes_count retweets_count  day  \n",
       "0   ['superleague', 'teamom']           5              1   21  \n",
       "1  ['marotta', 'superleague']          56             18   21  \n",
       "2    ['superleague', 'olasm']           6              0   21  \n",
       "3   ['superleague', 'teamom']           7              0   21  \n",
       "4             ['superleague']           2              2   21  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>22:29:44</td>\n",
       "      <td>Jean Michel incisif face aux 12 frondeurs de l...</td>\n",
       "      <td>jean michel incisif face frondeur teamom jamais</td>\n",
       "      <td>['superleague', 'teamom']</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>22:29:01</td>\n",
       "      <td>Beppe #Marotta (AD Inter) lance un cri d’alarm...</td>\n",
       "      <td>beppe marotta ad clubs_signataires lancer cri ...</td>\n",
       "      <td>['marotta', 'superleague']</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>22:27:10</td>\n",
       "      <td>Avec l'échec du projet de #SuperLeague, la mor...</td>\n",
       "      <td>echec projet mort visage olasm</td>\n",
       "      <td>['superleague', 'olasm']</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>22:26:00</td>\n",
       "      <td>Jean Michel trop heureux de la mort de la #Sup...</td>\n",
       "      <td>jean michel trop heureux mort teamom</td>\n",
       "      <td>['superleague', 'teamom']</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>22:25:48</td>\n",
       "      <td>3-1 pour la Juve contre Parme (2 Coupes UEFA) ...</td>\n",
       "      <td>clubs_signataires contre parme coupe uefa comp...</td>\n",
       "      <td>['superleague']</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création du Fichier Csv Final"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Création d'un nouveau fichier csv qu'on utilisera par la suite\n",
    "tweets.to_csv('../Data_clean/tweets_fr_clean.csv', index=False, header=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_venv",
   "language": "python",
   "name": "twitter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}